<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Floating point numbers"/>
    <title>Floating point numbers</title>
    <link rel="stylesheet" type="text/css" href="../../style.css" media="screen">
</head>

<body>

<nav>
	<ul>
		<li><a class="navbar" href="../../index.html">Home</a></li>
		<li><a class="navbar" href="index.html">Index</a></li>
	</ul>
</nav>

<div class="content">

<h1>Floating point numbers</h1>

<p>Floating-point numbers are commonly used in computer science as an easy-to-use and accurate approximation for real numbers. In the decimal number system we frequently represent very large or very small numbers in <i>scientific notation</i> rather than as a fixed point number. For example Planck's constant can be represented as</p>

<table class="noborder centertable">
<tr><td>6.63</td> <td>x</td> <td>10<sup>−34</sup></td></tr>
<tr><td>Mantissa</td> <td></td> <td>Exponent</td></tr>
</table>

<p>The power to which 10 is raised, such as −24 is called the <i>exponent</i> or <i>characteristic</i>, while the number in front is called the <i>mantissa</i>.</p>

<p>By substituting the base 2 for the base 10, we can use a similar notation for representing real numbers in a computer. For example, the decimal number 5.625 in binary 101.101  (see <a href="binary.html#example">example 2.</a>) could be represented as 1.01101 x 2<sup>2</sup> or 1011.01 x 2<sup>−1</sup>, where each exponent specifies the true position of the binary point relative to its current position in the mantissa. Becaues the binary point can be dynamically altered by adjusting the size of the exponent, we call this representation <i>floating point</i>.</p>

<p>The IEEE floating point standard uses 32 bits to store the number, out of which, 8 bits are used to store the exponent and 23 bits to store the mantissa and 1 bit to store the sign, '0' denotes a positive number and a '1' denotes a negative number. The exponent is represented in a <i>biased</i> form, by adding a constant to the trye exponent. If a number's exponent is 0, it is stored as 0 + 127 = 127. If the exponent is −2, it is stored as −2 + 127 = 125. The advantage of the biased representation of exponents is that the most negative exponent is represented by zero and that most positive is represented by all ones. The range is actually [-126,127] and exponent" 128 is reserved for infinities and NaNs. </p>

<p>The precision of a floing point number is about 7 decimal digits (actually the log base 10 of 2<sup>23</sup>, about 6.92 digits of precision). Notice that log<sub>10</sub>(100) == 2 "means" you need 2 digits to represent 100 possible values (from 0 to 99). </p>

<pre>S MMMMMMMMMMMMMMMMMMMMMMM EEEEEEEE</pre>

<p>The range of number we can represent with an 8-bit exponent is approximately 10<sup>−39</sup> to 10<sup>+39</sup> and the <i>precision</i> we get with a 7-bit mantissa is about 1 part in 10<sup>3</sup>.

<p><b>Example 1.3.1</b>. Taking the decimal number 85.5 as an example, it is converted into a binary number as 1010101.1, whcih can be counted as 1.0101011 x 2<sup>6</sup> by using the scientific notation of binary. The sign bit is "0" and the exponent "6" is stored by adding a bias 127 to it, i.e. 133, in binary 10000101. The mantissa bit "0101011" is appended with 0s to fit into 23 bits, which then becomes "010 1011 0000 0000 0000 0000". &emsp;■</p>

<h2 id="double">Double: IEEE 754 double-precision binary floating-point format; binary64</h2>

<p>Double-precision binary floating-point is a commonly used format on PCs, due to its wider range over single-precision floating point, in spite of its performance and bandwidth cost. As with single-precision floating-point format, it lacks precision on integer numbers when compared with an integer format of the same size. It is commonly known simply as <b>double</b>. The IEEE 754 standard specifies a binary64 as having:</p>

<ul>
    <li><p>Sign bit: 1 bit</p></li>
    <li><p>Exponent: 11 bits</p></li>
    <li><p>Significand precision: 53 bits (52 explicitly stored)</p></li>
</ul>

<img src="img/double.png" alt="double"/>

<p>On a typical computer system, a double-precision (64-bit) binary floating-point number has a coefficient of 53 bits (including 1 implied bit), an exponent of 11 bits, and 1 sign bit. Since 2<sup>10</sup> = 1024, the complete range of the positive normal floating-point numbers in this format is from 2<sup>−1022</sup> ≈ 2 × 10<sup>−308</sup> to approximately 2<sup>1024</sup> ≈ 2 × 10<sup>308</sup>. </p>

<p>The 53-bit significand precision gives from 15 to 17 significant decimal digits precision (2<sup>-53</sup> ≈ 1.11 × 10<sup>−16</sup>)</p>

<h2>Observations</h2>

<pre>#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

#ifndef PI
#define PI 3.141592653589793238462643383279502884L
#endif

#ifndef PIX
#define PIX 3.141592653589793238462643383279502884
#endif

int main() {

	long double a = PI;
	double pi = PIX;

	printf("%.22Lf\n", a);
	printf("%.22lf\n", pi);
	printf("size of a is %d Bytes\n", sizeof (double));
	printf("size of a double %d Bytes\n", sizeof (long double));
	
	return 0;
}
</pre>

<p>a long double is not 16 bytes, sizeof does not indicate how bit the implemented data type is. It "occupies" 16 bytes because the compiler chooses to, but it is 10 byte, log(1/10)/log(1/2) ⟶ 3.3, so 3 decimal positions ~= 10 bits</p>

<h2>Floating-point values comparison</h2>

<p>The relational operators &lt; and &gt; can be used with floating-point number, too. Beware, though that round-off erros can prevent two numbers from being equal, even though logically they should be. For example, the following expression, although algebrically true, will evaluate to false on most computers:</p>

<pre>(1.0/3.0 + 1.0/3.0 + 1.0/3.0) == 1.0</pre>

<p>This evaluates to 0 (false) becaues the fraction 1.0/3.0 contains an infinite number of decimal places (0.333333..). The computer is capable of holding only a limited number of decimal places, so it rounds each occurence of 1/3. As a result, the left-hand side of the expression does not equal 1.0 exactly.</p>

<p>Because floating-point numbers are so tricky to deal with, it's generally bad practice to compare a floating-point number for equality with anything. Instead inequalities are much safer. The <code>fabs()</code> function, declared in math.h header file, can be handy for floating-point comparisons. This functions returns the absolute value of a floating-point value − that is, the value without the algebraic sing. For example, if two numbers are 3.1415 and 3.1410 then they are same up to the precision 0.001, but after that, like 0.0001 they are not same.</p>

<pre>
#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

int main() 
{
	float a = 3.1415;
	float b = 3.1410; 
	float epsilon = 0.001;

	if(fabs(a-b) &lt; epsilon)
		printf("They are equal\n");

	else
		printf("They are different\n");

	return 0;
}
</pre>

<p>output</p>

<pre>they are equal</pre>

<section aria-label="End">
<a href="system.html">&#171;Thermodynamic Systems</a>
<a href="index.html">Index</a>
<a href="continuity.html">Definizione limite per funzioni operanti tra spazi metrici o normati &#187;</a>
</section>

</div>

</body>
</html>
